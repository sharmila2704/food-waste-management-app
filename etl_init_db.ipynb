{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f1922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544af6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.path.dirname(__file__)\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "DB_PATH = os.path.join(ROOT, \"foodwaste.db\")\n",
    "SCHEMA_PATH = os.path.join(ROOT, \"schema.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de26f35",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "PROVIDERS_CSV = os.path.join(DATA_DIR, \"providers_data.csv\")\n",
    "RECEIVERS_CSV = os.path.join(DATA_DIR, \"receivers_data.csv\")\n",
    "LISTINGS_CSV  = os.path.join(DATA_DIR, \"food_listings_data.csv\")\n",
    "CLAIMS_CSV    = os.path.join(DATA_DIR, \"claims_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fcc8e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _ensure_dummy_data_if_missing():\n",
    "    \"\"\"Generate a tiny dummy dataset if CSVs aren't present, so the app can run end-to-end.\"\"\"\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    if not os.path.exists(PROVIDERS_CSV):\n",
    "        pd.DataFrame([\n",
    "            {\"Provider_ID\": 1, \"Name\": \"Green Bites\", \"Type\": \"Restaurant\", \"Address\": \"12 MG Road\", \"City\": \"Bengaluru\", \"Contact\": \"99999-11111\"},\n",
    "            {\"Provider_ID\": 2, \"Name\": \"FreshMart\", \"Type\": \"Grocery Store\", \"Address\": \"45 Anna Salai\", \"City\": \"Chennai\", \"Contact\": \"99999-22222\"},\n",
    "        ]).to_csv(PROVIDERS_CSV, index=False)\n",
    "    if not os.path.exists(RECEIVERS_CSV):\n",
    "        pd.DataFrame([\n",
    "            {\"Receiver_ID\": 1, \"Name\": \"Helping Hands NGO\", \"Type\": \"NGO\", \"City\": \"Bengaluru\", \"Contact\": \"88888-11111\"},\n",
    "            {\"Receiver_ID\": 2, \"Name\": \"City Shelter\", \"Type\": \"Shelter\", \"City\": \"Chennai\", \"Contact\": \"88888-22222\"},\n",
    "        ]).to_csv(RECEIVERS_CSV, index=False)\n",
    "    if not os.path.exists(LISTINGS_CSV):\n",
    "        today = datetime.utcnow().date()\n",
    "        pd.DataFrame([\n",
    "            {\"Food_ID\": 101, \"Food_Name\": \"Veg Biryani\", \"Quantity\": 20, \"Expiry_Date\": str(today+timedelta(days=1)),\n",
    "             \"Provider_ID\": 1, \"Provider_Type\": \"Restaurant\", \"Location\": \"Bengaluru\", \"Food_Type\": \"Vegetarian\", \"Meal_Type\": \"Lunch\"},\n",
    "            {\"Food_ID\": 102, \"Food_Name\": \"Bread Loaves\", \"Quantity\": 50, \"Expiry_Date\": str(today+timedelta(days=2)),\n",
    "             \"Provider_ID\": 2, \"Provider_Type\": \"Grocery Store\", \"Location\": \"Chennai\", \"Food_Type\": \"Vegan\", \"Meal_Type\": \"Breakfast\"},\n",
    "        ]).to_csv(LISTINGS_CSV, index=False)\n",
    "    if not os.path.exists(CLAIMS_CSV):\n",
    "        now = datetime.utcnow()\n",
    "        pd.DataFrame([\n",
    "            {\"Claim_ID\": 1001, \"Food_ID\": 101, \"Receiver_ID\": 1, \"Status\": \"Completed\", \"Timestamp\": str(now)},\n",
    "            {\"Claim_ID\": 1002, \"Food_ID\": 101, \"Receiver_ID\": 1, \"Status\": \"Pending\",   \"Timestamp\": str(now)},\n",
    "            {\"Claim_ID\": 1003, \"Food_ID\": 102, \"Receiver_ID\": 2, \"Status\": \"Cancelled\", \"Timestamp\": str(now)},\n",
    "        ]).to_csv(CLAIMS_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93550cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _read_csvs():\n",
    "    providers = pd.read_csv(PROVIDERS_CSV, dtype={\"Provider_ID\": \"Int64\"}, keep_default_na=False)\n",
    "    receivers = pd.read_csv(RECEIVERS_CSV, dtype={\"Receiver_ID\": \"Int64\"}, keep_default_na=False)\n",
    "    listings  = pd.read_csv(LISTINGS_CSV, dtype={\"Food_ID\": \"Int64\", \"Provider_ID\": \"Int64\"}, keep_default_na=False)\n",
    "    claims    = pd.read_csv(CLAIMS_CSV, dtype={\"Claim_ID\": \"Int64\", \"Food_ID\": \"Int64\", \"Receiver_ID\": \"Int64\"}, keep_default_na=False)\n",
    "\n",
    "    # Parse date columns\n",
    "    if \"Expiry_Date\" in listings.columns:\n",
    "        listings[\"Expiry_Date\"] = pd.to_datetime(listings[\"Expiry_Date\"], errors=\"coerce\").dt.date\n",
    "    if \"Timestamp\" in claims.columns:\n",
    "        claims[\"Timestamp\"] = pd.to_datetime(claims[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    # Deduplicate on primary keys\n",
    "    providers = providers.drop_duplicates(subset=[\"Provider_ID\"]).dropna(subset=[\"Provider_ID\"])\n",
    "    receivers = receivers.drop_duplicates(subset=[\"Receiver_ID\"]).dropna(subset=[\"Receiver_ID\"])\n",
    "    listings  = listings.drop_duplicates(subset=[\"Food_ID\"]).dropna(subset=[\"Food_ID\"])\n",
    "    claims    = claims.drop_duplicates(subset=[\"Claim_ID\"]).dropna(subset=[\"Claim_ID\"])\n",
    "\n",
    "    # Enforce integer types after dropping NA\n",
    "    providers[\"Provider_ID\"] = providers[\"Provider_ID\"].astype(int)\n",
    "    receivers[\"Receiver_ID\"] = receivers[\"Receiver_ID\"].astype(int)\n",
    "    listings[\"Food_ID\"] = listings[\"Food_ID\"].astype(int)\n",
    "    listings[\"Provider_ID\"] = listings[\"Provider_ID\"].astype(int)\n",
    "    claims[\"Claim_ID\"] = claims[\"Claim_ID\"].astype(int)\n",
    "    claims[\"Food_ID\"] = claims[\"Food_ID\"].astype(int)\n",
    "    claims[\"Receiver_ID\"] = claims[\"Receiver_ID\"].astype(int)\n",
    "\n",
    "    # Referential integrity: keep only valid links\n",
    "    valid_provider_ids = set(providers[\"Provider_ID\"])\n",
    "    listings = listings[listings[\"Provider_ID\"].isin(valid_provider_ids)].copy()\n",
    "\n",
    "    valid_food_ids = set(listings[\"Food_ID\"])\n",
    "    valid_receiver_ids = set(receivers[\"Receiver_ID\"])\n",
    "    claims = claims[claims[\"Food_ID\"].isin(valid_food_ids) & claims[\"Receiver_ID\"].isin(valid_receiver_ids)].copy()\n",
    "\n",
    "    # Sanitise categorical values to match schema checks\n",
    "    def normalize(value, allowed, default):\n",
    "        v = str(value).strip()\n",
    "        return v if v in allowed else default\n",
    "\n",
    "    if \"Type\" in providers.columns:\n",
    "        providers[\"Type\"] = providers[\"Type\"].map(lambda v: normalize(v, {'Restaurant','Grocery Store','Supermarket','Bakery','Caterer','Other'}, 'Other'))\n",
    "    if \"Type\" in receivers.columns:\n",
    "        receivers[\"Type\"] = receivers[\"Type\"].map(lambda v: normalize(v, {'NGO','Community Center','Individual','Shelter','Other'}, 'Other'))\n",
    "    if \"Food_Type\" in listings.columns:\n",
    "        listings[\"Food_Type\"] = listings[\"Food_Type\"].map(lambda v: normalize(v, {'Vegetarian','Non-Vegetarian','Vegan','Other'}, 'Other'))\n",
    "    if \"Meal_Type\" in listings.columns:\n",
    "        listings[\"Meal_Type\"] = listings[\"Meal_Type\"].map(lambda v: normalize(v, {'Breakfast','Lunch','Dinner','Snacks','Other'}, 'Other'))\n",
    "    if \"Status\" in claims.columns:\n",
    "        claims[\"Status\"] = claims[\"Status\"].map(lambda v: normalize(v, {'Pending','Completed','Cancelled'}, 'Pending'))\n",
    "\n",
    "    return providers, receivers, listings, claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59340f8d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _run_schema(conn):\n",
    "    with open(SCHEMA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        sql = f.read()\n",
    "    conn.executescript(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a0908",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_database():\n",
    "    _ensure_dummy_data_if_missing()\n",
    "\n",
    "    providers, receivers, listings, claims = _read_csvs()\n",
    "\n",
    "    if os.path.exists(DB_PATH):\n",
    "        os.remove(DB_PATH)\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH, detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES)\n",
    "    try:\n",
    "        conn.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "        _run_schema(conn)\n",
    "\n",
    "        # Load data\n",
    "        providers.to_sql(\"providers\", conn, if_exists=\"append\", index=False)\n",
    "        receivers.to_sql(\"receivers\", conn, if_exists=\"append\", index=False)\n",
    "        listings.to_sql(\"food_listings\", conn, if_exists=\"append\", index=False)\n",
    "        claims.to_sql(\"claims\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "        # Quick counts\n",
    "        cur = conn.cursor()\n",
    "        tables = [\"providers\",\"receivers\",\"food_listings\",\"claims\"]\n",
    "        counts = {t: cur.execute(f\"SELECT COUNT(*) FROM {t};\").fetchone()[0] for t in tables}\n",
    "        return {\"db_path\": DB_PATH, \"counts\": counts}\n",
    "    finally:\n",
    "        conn.commit()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    info = build_database()\n",
    "    print(\"Database built:\", info)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
